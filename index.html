<!DOCTYPE html>
<html class=" w-mod-ix"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"><style>.wf-force-outline-none[tabindex="-1"]:focus{outline:none;}</style>
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-YMTSZM1Z57"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-YMTSZM1Z57');
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>Text2Place : Affordance-aware Text Guided Human Placement</title>
    <link rel="stylesheet" href="images/bootstrap.min.css">
    <link href="images/css.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="images/Highlight-Clean.css">
    <link rel="stylesheet" href="images/styles.css">

    <link rel="manifest" href="images/site.webmanifest">

    <meta name="description" content="Text2Place : Affordance-aware Text Guided Human Placement, 2023.">
    <meta property="og:url" content="https://Harsh-Gupta9897.github.io">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Text2Place : Affordance-aware Text Guided Human Placement">
    <meta property="og:description" content="Text2Place : Affordance-aware Text Guided Human Placement, 2023.">
    <meta property="og:image" content="https://Harsh-Gupta9897.github.io/static/main_teaser.png">

    <!-- Twitter Meta Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta property="twitter:domain" content="github.io">
    <meta property="twitter:url" content="https://Harsh-Gupta9897.github.io">
    <meta name="twitter:title" content="Text2Place : Affordance-aware Text Guided Human Placement">
    <meta name="twitter:description" content="Text2Place : Affordance-aware Text Guided Human Placement, 2023.">
    <meta name="twitter:image" content="https://Harsh-Gupta9897.github.io/static/main_teaser.png">


    <script src="images/video_comparison.js"></script>
    <script type="module" src="images/model-viewer.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        strong {
          font-weight: bold;
        }
    </style>

  
</head>

<body>

    
    <div class="highlight-clean d-flex flex-column align-items-center justify-content-center" style="padding-bottom: 20px;">
        <div class="container" style="padding-bottom: 10px; max-width: 1000px;">
            <h1 class="text-center">Text2Place : Affordance-aware Text Guided Human Placement</h1>
        </div>
        <div class="container  d-flex flex-column align-items-center justify-content-center" style="max-width: 900px;">
            <div class="row authors" style="padding-bottom: 20px;">
                <h5 class="text-center"><a class="text-center" href="https://nupurkmr9.github.io">Rishubh Parihar</a><sup>1*</sup></h5>
                <h5 class="text-center"><a class="text-center" href=" https://graceduansu.github.io">Harsh Gupta</a><sup>1*</sup></h5>
                <h5 class="text-center"><a href="https://richzhang.github.io">Sachidanand</a><sup>1</sup></h5>
                <h5 class="text-center"><a href="https://taesung.me">R.Venkatesh Babu</a><sup>1</sup></h5>
            </div>    
            <div class="row authors institute">
                <div class="col-sm-12">
                    <h3 class="text-center"><sup>1</sup> Indian Institute of Science, Bengalaru </h3>
                </div>
            </div>    
        </div>
        <div class="buttons" style="margin-bottom: 8px;">
            <a class="btn btn-light" role="button" href="#">
                <svg style="visibility:hidden;width:0px;height:24px;margin-left:-12px;margin-right:12px" width="0px" height="24px" viewBox="0 0 375 531">
                    <polygon stroke="#000000" points="0.5,0.866 459.5,265.87 0.5,530.874 "/>
                </svg>
                Code
            </a>
            <a class="btn btn-light" role="button" href="#">
                <svg style="width:24px;height:24px;margin-left:-12px;margin-right:12px" viewBox="0 0 24 24">
                    <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z"></path>
                </svg>Paper
            </a>
            <a class="btn btn-light" role="button" href="#">
                <svg style="width:24px;height:24px;margin-left:-12px;margin-right:12px" viewBox="0 0 24 24">
                    <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z"></path>
                </svg>Arxiv
            </a>
            <a class="btn btn-light disabled border border-dark" aria-disabled="true" role="button" href="#">
                <svg style="visibility:hidden;width:0px;height:24px;margin-left:-12px;margin-right:12px" width="0px" height="24px" viewBox="0 0 375 531">
                    <polygon stroke="#000000" points="0.5,0.866 459.5,265.87 0.5,530.874 "/>
                </svg>
                Project
            </a>

             <!-- <a class="btn btn-light" role="button" href="https://huggingface.co/spaces/customdiffusion360/customdiffusion360">
                <svg style="width:24px;height:24px;margin-left:-12px;margin-right:12px" viewBox="0 0 375 531">
                    <img src="images/huggingface_logo-noborder.svg" style="width:24px;height:24px;margin-left:-12px;margin-right:12px"/>
                </svg>
                Demo
            </a> -->
        </div>
    </div>

    <div class="container" style="max-width: 1200px;">
        <div class="row teaser">
            <div class="col-md-12">
                <!-- Large format devices -->
                <center><img src="./static/main_teaser.png" style="width: 90%;"/></center>
                <p>
                <strong>(Top)</strong>: Proposed approach for text-based placement of humans. Given a background image, we predict the plausible semantic region compatible with the text prompt to place humans. Next, given a few subject images, we perform subject-conditioned inpainting to realistically place humans in appropriate poses following the scene affordances. <strong>(Bottom)</strong>: Our method enables <strong>a)</strong> realistic human placements at diverse locations and poses and several downstream applications <strong>b)</strong> scene hallucination by generating compatible scenes for the given pose of the human <strong>c)</strong> text-based editing of the human and <strong>d)</strong> placing multiple persons.
                Notably, our method stands as the first of its kind to achieve this precision solely through textual descriptions of the scene.
                </p><p></p>
                <h6 class="caption"></h6>
            </div>
        </div>
    </div>


    <!-- <hr class="divider"> -->
    <hr style="max-width: 1200px;">
    <div class="container" style="max-width: 1200px;">
        <div class="row">
            <div class="col-md-12">
                <center><h2>Abstract</h2></center>
                <!-- <h2>Abstract</h2> -->
                <p>
                    For a given scene, humans can easily reason for the locations and pose to place objects. Designing a computational model to reason about these affordances poses a significant challenge, mirroring the intuitive reasoning abilities of humans. This work tackles the problem of realistic human insertion in a given background scene termed as <strong>Semantic Human Placement</strong>. This task is extremely challenging given the diverse backgrounds, scale, and pose of the generated person and, finally, the identity preservation of the person. We divide the problem into the following two stages  <strong>i)</strong> learning <em>semantic masks</em> using text guidance for localizing regions in the image to place humans and <strong>ii)</strong> subject-conditioned inpainting to place a given subject adhering to the scene affordance within the <em>semantic masks</em>. For learning semantic masks, we leverage rich object-scene priors learned from the text-to-image generative models and optimize a novel parameterization of the semantic mask, eliminating the need for large-scale training. To the best of our knowledge, we are the first ones to provide an effective solution for realistic human placements in diverse real-world scenes. The proposed method can generate highly realistic scene compositions while preserving the background and subject identity. Further, we present results for several downstream tasks - scene hallucination from a single or multiple generated persons and text-based attribute editing. With extensive comparisons against strong baselines, we show the superiority of our method in realistic human placement. 
                </p>
                
            </div>
        </div>
    </div>

   

    <!-- <hr class="divider"> -->
    <hr style="max-width: 1200px;">
    <div class="container" style="max-width: 1200px;">
        <div class="row">
            <div class="col-md-12">
                <center><h2>Methodology</h2></center>
                <p>
                Our approach consists of two stages: <strong>a) Semantic Mask Optimization.</strong> Given a background image \( \mathcal{I}_b \), we initialize a blob mask \( \mathcal{M} \) parameterized as Gaussian blobs and a foreground person image \( \mathcal{I}_p \). These two images are combined to form a composite image \( \mathcal{I}_c \), which is used to compute SDS loss with the action prompt. During optimization, only \( \mathcal{M} \) and \( \mathcal{I}_p \) are getting updated via \( \mathcal{I}_c \). After training \( \mathcal{M} \) converge to a plausible human placement region, which is then used for inpainting. <strong>b) Subject conditioned inpainting.</strong> Given a few subject images, we perform Textual Inversion to obtain its token embedding \( \mathbf{V*} \). Next, we use the inpainting pipeline of T2I models to perform personalized inpainting of the subject. 
                </p>
                <p></p><p></p>
                <div style="text-align: center;">
                    <img src="./static/methodology.png" style="width: 100%;"/>
                  </div>
                <p></p><p></p>

            </div>
        </div>
    </div>

  
    <!-- <hr class="divider"> -->
    <hr style="max-width: 1200px;">
    <div class="container" style="max-width: 1200px;">
        <div class="row">
            <div class="col-md-12">
                <center><h2>Comparison to Baselines</h2></center>
                Baseline Comparison: Notably, <strong>LLaVA</strong> generates masks with semantically in-
                correct locations, while <strong>GPT4v</strong> produces excessively large mask sizes. <strong>SDEdit</strong>  distorts
                the image, whereas our method accurately determines the optimal location and size
                for person insertion.
                <p></p><p></p>
                <div style="text-align: center;">
                    <img src="static/baselines.png" style="width: 90%;"/>
                  </div>

                <div id="singleconceptscroll" class="container" style="max-width: 1200px; ">

                <br>
                </div>
            </div>
        </div>
        <hr style="max-width: 1000px;">
        <div class="row">
            <div class="col-md-12">
                <center><h2>Results</h2></center>
                We had shown results on in-wild 30 real as well as non-real scenes with different personalitites embedded with the help of Textual Inversion to get the results.
                <p></p><p></p>
                <div id="singleconceptscroll" class="container" style="max-width: 1200px; ">
                <br>
                <div style="text-align: center;">
                    <img src="static/results.png" style="width: 95%;"/>
                </div>
                
                </div>
            </div>
        </div>

        <hr style="max-width: 1000px;">

    </div>

    
    <hr style="max-width: 1200px;">
    <div class="container" style="max-width: 1200px;">
        <div class="row">
            <div class="col-md-12">
                <center><h2>Applications </h2></center>
                <p></p><p></p>
                <div id="composition" class="container" style="max-width: 1200px;">
                    <center> <img  src="static/application2.png" style="width: 85%; margin-left: auto; margin-right: auto;" /> </center>
                        
                </div>
                <br>
                <p></p><p></p>   
                <div id="composition" class="container" style="max-width: 1200px;">
                    <center> <img  src="static/application3.png" style="width: 85%; margin-left: auto; margin-right: auto;" /> </center>
                        
                </div>
                <br>
                <p></p><p></p>   
                <div id="composition" class="container" style="max-width: 1200px;">
                    <center> <img  src="static/application4.png" style="width: 85%; margin-left: auto; margin-right: auto;" /> </center>
                        
                </div>
                <br>
                <p></p><p></p>   


                <div id="composition" class="container" style="max-width: 1200px;">
                    <center> <img  src="static/application5.png" style="width: 85%; margin-left: auto; margin-right: auto;" /> </center>
                        
                </div>
                <br>   

                <p></p><p></p>

            </div>
        </div>
    </div>

    <hr style="max-width: 1200px;">
    <div class="container" style="max-width: 1200px;">
        <div class="row">
            <div class="col-md-12">
                <center><h2>Limitations </h2></center>
                <p> Our approach fails in placing small objects due to relatively bigger semantic masks resulting in background changes or placing objects of inappropriate size.
                <p></p><p></p>
                <div id="limitation" class="container" style="max-width: 1200px;">

                    <center> <img  src="static/limitation.png" style="width: 70%; margin-left: auto; margin-right: auto;" /> </center>
                        
                </div>
                <p></p><p></p>

            </div>
        </div>
    </div>



    <hr style="max-width: 1200px;">
    <div class="container" style="max-width: 1200px;">
        <!-- <div class="row">
            <div class="col-md-12">
                <center><h2>Citation</h2></center>
                <code>
                    @article{kumari2024customdiffusion360,<br>
                    &nbsp; author = {Kumari, Nupur and Su, Grace and Zhang, Richard and Park, Taesung and Shechtman, Eli and Zhu, Jun-Yan},<br>
                    &nbsp; title  = {Text2Place : Affordance-aware Text Guided Human Placement},<br>
                    &nbsp; journal = {Arxiv},<br>
                    &nbsp; year   = {2024},<br>
                }</code>
            </div>
        </div> -->
    </div>

    <hr style="max-width: 1200px;">
    <!-- <div class="container" style="max-width: 1200px;">
        <div class="row">
            <div class="col-md-12">
                <center><h2>Concurrent Works</h2></center>
                <h5>
                <ul>

                    <li> Ta-Ying Cheng, Matheus Gadelha, Thibault Groueix, Matthew Fisher, Radomir Mech, Andrew Markham, and Niki Trigoni. <a href="https://ttchengab.github.io/continuous_3d_words">Learning Continuous 3D Words for Text-to-Image Generation.</a> CVPR 2024.</li><br>

                    <li> Lukas Höllein, Aljaž Božič, Norman Müller, David Novotny, Hung-Yu Tseng, Christian Richardt, Michael Zollhöfer, and Matthias Nießner. <a href="https://lukashoel.github.io/ViewDiff/">Viewdiff: 3d-consistent image generation with text-to-image models.</a> CVPR 2024.</li><br>

                </ul> 
            </h5>               
            </div>
        </div>
    </div> -->

    <!-- <hr style="max-width: 1200px;">
    <div class="container" style="max-width: 1200px;">
        <div class="row">
            <div class="col-md-12">
                <center><h2>Acknowledgements</h2></center>
                <p> We are thankful to Kangle Deng, Sheng-Yu Wang, and Gaurav Parmar for their helpful comments and discussion and to Sean Liu, Ruihan Gao, Yufei Ye, and Bharath Raj for proofreading the draft. This work was partly done by Nupur Kumari during the Adobe internship. The work is partly supported by Adobe Research, the Packard Fellowship, the Amazon Faculty Research Award, and NSF IIS-2239076. Grace Su is supported by the NSF Graduate Research Fellowship (Grant No. DGE2140739).
                </p>
            </div>
        </div>
    </div> -->






    <script src="images/polyfill.js"></script>
    <script src="images/yall.js"></script>
    <script>
        yall(
            {
                observeChanges: true
            }
        );
    </script>
    <script src="images/scripts.js"></script>
    <script src="images/jquery.min.js"></script>
    <script src="images/bootstrap.bundle.min.js"></script>
    <script src="images/webflow.fd002feec.js"></script>
    
    <!-- Import the component -->



</body></html>